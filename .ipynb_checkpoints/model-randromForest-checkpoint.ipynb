{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f85b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.0\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - flask\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    flask-1.1.2                |     pyh9f0ad1d_0          70 KB  conda-forge\n",
      "    itsdangerous-1.1.0         |             py_0          16 KB  conda-forge\n",
      "    werkzeug-1.0.1             |     pyh9f0ad1d_0         239 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         324 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  flask              conda-forge/noarch::flask-1.1.2-pyh9f0ad1d_0\n",
      "  itsdangerous       conda-forge/noarch::itsdangerous-1.1.0-py_0\n",
      "  werkzeug           conda-forge/noarch::werkzeug-1.0.1-pyh9f0ad1d_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "flask-1.1.2          | 70 KB     | ##################################### | 100% \n",
      "itsdangerous-1.1.0   | 16 KB     | ##################################### | 100% \n",
      "werkzeug-1.0.1       | 239 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "0.6538461538461539\n",
      "['104' '108' '100' '100' '4' '220' '211' '4' '4' '1' '0' '100' '220' '211'\n",
      " '100' '108' '211' '118' '207' '100' '211' '4' '113' '211' '220' '211']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "clf = RandomForestClassifier(n_estimators = 100) \n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from sklearn import tree\n",
    "try:\n",
    "     import flask\n",
    "except:\n",
    "    ! conda install --yes flask         \n",
    "\n",
    "from flask import Flask, request, redirect, url_for, flash, jsonify\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "def load (input):\n",
    "    \n",
    "    data = pd.read_csv(input, delimiter = \",\")\n",
    "    extraced_ssid_level_pivot =data.pivot(columns=['bssid'], values='level')\n",
    "\n",
    "    output =pd.concat([data, extraced_ssid_level_pivot] , axis=1)\n",
    "    if 'level' in output:\n",
    "        del output['level']\n",
    "    if 'ssid' in output:\n",
    "        del output['ssid']\n",
    "    if 'bssid' in output:\n",
    "        del output['bssid']\n",
    "    return output\n",
    "\n",
    "def get_features_and_target(df , target_to_predicte):\n",
    "    target=df[target_to_predicte]\n",
    "    if target_to_predicte in df:\n",
    "        del df[target_to_predicte]\n",
    "    features=df\n",
    "    return features, target\n",
    "\n",
    "dataset  = load('donnees.txt') ;   # on souhaitera ajouter des donnees dans ce dataset\n",
    "dataset['salleid'] = dataset['salleid'].astype(str)\n",
    "\n",
    "dataset = dataset.groupby('positionid').agg(\"max\").fillna(100)\n",
    "dataset\n",
    "\n",
    "# 8\n",
    "X, y =get_features_and_target(dataset, 'salleid')\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33)\n",
    "clf = RandomForestClassifier(criterion='gini', max_depth=8,n_estimators=50)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred =clf.predict(X_test)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de26be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "res  = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40c5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_labels():\n",
    "    labels = []\n",
    "    for element in y_train:\n",
    "        if element not in labels:\n",
    "            labels.append(element)\n",
    "\n",
    "    labels  = np.array(labels).astype(\"int\")\n",
    "    labels.sort()\n",
    "    return labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a603a70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 100, 104, 108, 113, 118, 207, 211, 214, 218, 220]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = get_labels()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed24fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02, 0.03, 0.08, 0.5476666666666667, 0.17233333333333334, 0.02, 0.05, 0.0, 0.02, 0.02, 0.0, 0.01, 0.03]\n",
      "3\n",
      "[0.02, 0.03, 0.08, -1, 0.17233333333333334, 0.02, 0.05, 0.0, 0.02, 0.02, 0.0, 0.01, 0.03]\n",
      "4\n",
      "[0.02, 0.03, 0.08, -1, -1, 0.02, 0.05, 0.0, 0.02, 0.02, 0.0, 0.01, 0.03]\n",
      "2\n",
      "{100: 54.76666666666667, 104: 17.233333333333334, 4: 8.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# recupere les trois max et leur index   cle valeur \n",
    "def get_max(predicted_proba , labels):\n",
    "    dic = dict()\n",
    "    for i in range(3):\n",
    "        print(predicted_proba)\n",
    "        max_value = max(predicted_proba)\n",
    "        max_index = predicted_proba.index(max_value)\n",
    "        print(max_index)\n",
    "        key = labels[max_index]\n",
    "        dic[key] = max_value*100\n",
    "        predicted_proba[max_index]=-1\n",
    "    return dic\n",
    "\n",
    "#print(labels)\n",
    "print(get_max(res[0].tolist() ,  get_labels()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad54ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('modelfinal.pkl','wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423489c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input(dataset , X_dict): \n",
    "    X_dataframe = pd.DataFrame.from_dict(X_dict)\n",
    "    for feature in X_dataframe: \n",
    "        if feature in dataset:\n",
    "            print(feature)\n",
    "        else:\n",
    "            del X_dataframe[feature]\n",
    "    return  X_dataframe\n",
    "\n",
    "def threat_input(dataset , X_input):\n",
    "    X_input= clean_input(dataset , X_input)\n",
    "    key_array = []\n",
    "    for feature in X_input:\n",
    "        key_array.append(feature)\n",
    "    values = X_input.values\n",
    "    maj_dataset =  dataset.append(dict(zip(key_array, *values)), ignore_index=True)\n",
    "    formated_input = maj_dataset.replace(np.nan,100)\n",
    "    formated_input = formated_input[-1:].values\n",
    "    return formated_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['GET','POST'])\n",
    "def makecalc():\n",
    "    #print(request.json)\n",
    "    #print(dataset)\n",
    "    observation = threat_input(dataset , request.json)\n",
    "    #print(threat_input(dataset , request.json))\n",
    "    print (\"predicted\" , model.predict(observation) , type(model.predict(observation)))\n",
    "    proba  = model.predict_proba(observation)\n",
    "    print (\"result\" , get_max(proba[0].tolist()  , labels))\n",
    "    return json.dumps(get_max(proba[0].tolist()  , labels))\n",
    "@app.route('/app', methods=['GET','POST'])\n",
    "def makecalcc():\n",
    "    print(request.json)\n",
    "    return \"computed\"\n",
    "if __name__ == '__main__':\n",
    "    modelfile = 'modelfinal.pkl'\n",
    "    model = p.load(open(modelfile, 'rb'))\n",
    "    app.run(debug=False, host='0.0.0.0')\n",
    "# ici le serveur traite les donnees qu'il recoit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "     import pandas_profiling\n",
    "except:\n",
    "    ! conda install --yes pandas-profiling   \n",
    "try:\n",
    "     import geopandas\n",
    "except:\n",
    "    ! conda install --yes geopandas    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34cfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
